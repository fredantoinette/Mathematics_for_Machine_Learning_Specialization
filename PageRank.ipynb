{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank\n",
    "In this notebook, we'll build on our knowledge of eigenvectors and eigenvalues by exploring the PageRank algorithm.\n",
    "The notebook is in two parts, the first is a worksheet to get us up to speed with how the algorithm works - here we will look at a micro-internet with fewer than 10 websites and see what it does and what can go wrong.\n",
    "The second is an assessment which will test our application of eigentheory to this problem by writing code and calculating the page rank of a large network representing a sub-section of the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Worksheet\n",
    "### Introduction\n",
    "\n",
    "PageRank (developed by Larry Page and Sergey Brin) revolutionized web search by generating a\n",
    "ranked list of web pages based on the underlying connectivity of the web. The PageRank algorithm is\n",
    "based on an ideal random web surfer who, when reaching a page, goes to the next page by clicking on a\n",
    "link. The surfer has equal probability of clicking any link on the page and, when reaching a page with no\n",
    "links, has equal probability of moving to any other page by typing in its URL. In addition, the surfer may\n",
    "occasionally choose to type in a random URL instead of following the links on a page. The PageRank is\n",
    "the ranked order of the pages from the most to the least probable page the surfer will be viewing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Before we begin, let's load the libraries.\n",
    "%pylab notebook\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from PageRankFunctions import *\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank as a linear algebra problem\n",
    "Let's imagine a micro-internet, with just 6 websites (**A**vocado, **B**ullseye, **C**atBabel, **D**romeda, **e**Tings, and **F**aceSpace).\n",
    "Each website links to some of the others, and this forms a network as shown,\n",
    "\n",
    "![A Micro-Internet](internet.png \"A Micro-Internet\")\n",
    "\n",
    "The design principle of PageRank is that important websites will be linked to by important websites.\n",
    "This somewhat recursive principle will form the basis of our thinking.\n",
    "\n",
    "Imagine we have 100 *Procrastinating Pat*s on our micro-internet, each viewing a single website at a time.\n",
    "Each minute the Pats follow a link on their website to another site on the micro-internet.\n",
    "After a while, the websites that are most linked to will have more Pats visiting them, and in the long run, each minute for every Pat that leaves a website, another will enter keeping the total numbers of Pats on each website constant.\n",
    "The PageRank is simply the ranking of websites by how many Pats they have on them at the end of this process.\n",
    "\n",
    "We represent the number of Pats on each website with the vector,\n",
    "$$\\mathbf{r} = \\begin{bmatrix} r_A \\\\ r_B \\\\ r_C \\\\ r_D \\\\ r_E \\\\ r_F \\end{bmatrix}$$\n",
    "And say that the number of Pats on each website in minute $i+1$ is related to those at minute $i$ by the matrix transformation\n",
    "\n",
    "$$ \\mathbf{r}^{(i+1)} = L \\,\\mathbf{r}^{(i)}$$\n",
    "with the matrix $L$ taking the form,\n",
    "$$ L = \\begin{bmatrix}\n",
    "L_{A→A} & L_{B→A} & L_{C→A} & L_{D→A} & L_{E→A} & L_{F→A} \\\\\n",
    "L_{A→B} & L_{B→B} & L_{C→B} & L_{D→B} & L_{E→B} & L_{F→B} \\\\\n",
    "L_{A→C} & L_{B→C} & L_{C→C} & L_{D→C} & L_{E→C} & L_{F→C} \\\\\n",
    "L_{A→D} & L_{B→D} & L_{C→D} & L_{D→D} & L_{E→D} & L_{F→D} \\\\\n",
    "L_{A→E} & L_{B→E} & L_{C→E} & L_{D→E} & L_{E→E} & L_{F→E} \\\\\n",
    "L_{A→F} & L_{B→F} & L_{C→F} & L_{D→F} & L_{E→F} & L_{F→F} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where the columns represent the probability of leaving a website for any other website, and sum to one.\n",
    "The rows determine how likely we are to enter a website from any other, though these need not add to one.\n",
    "The long time behaviour of this system is when $ \\mathbf{r}^{(i+1)} = \\mathbf{r}^{(i)}$, so we'll drop the superscripts here, and that allows us to write,\n",
    "$$ L \\,\\mathbf{r} = \\mathbf{r}$$\n",
    "\n",
    "which is an eigenvalue equation for the matrix $L$, with eigenvalue 1 (this is guaranteed by the probabalistic structure of the matrix $L$).\n",
    "\n",
    "Complete the matrix $L$ below, we've left out the column for which websites the *FaceSpace* website (F) links to.\n",
    "Remember, this is the probability to click on another website from this one, so each column should add to one (by scaling by the number of links)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.array([[0,   1/2, 1/3, 0, 0,   0 ],\n",
    "              [1/3, 0,   0,   0, 1/2, 0 ],\n",
    "              [1/3, 1/2, 0,   1, 0,   1/2 ],\n",
    "              [1/3, 0,   1/3, 0, 1/2, 1/2 ],\n",
    "              [0,   0,   0,   0, 0,   0 ],\n",
    "              [0,   0,   1/3, 0, 0,   0 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we could use a linear algebra library, as below, to calculate the eigenvalues and vectors.\n",
    "And this would work for a small system. But this gets unmanagable for large systems.\n",
    "And since we only care about the principal eigenvector (the one with the largest eigenvalue, which will be 1 in this case), we can use the *power iteration method* which will scale better, and is faster for large systems.\n",
    "\n",
    "Use the code below to peek at the PageRank for this micro-internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.        ,  5.33333333, 40.        , 25.33333333,  0.        ,\n",
       "       13.33333333])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eVals, eVecs = la.eig(L) # Gets the eigenvalues and vectors\n",
    "order = np.absolute(eVals).argsort()[::-1] # Orders them by their eigenvalues\n",
    "eVals = eVals[order]\n",
    "eVecs = eVecs[:,order]\n",
    "\n",
    "r = eVecs[:, 0] # Sets r to be the principal eigenvector\n",
    "100 * np.real(r / np.sum(r)) # Make this eigenvector sum to one, then multiply by 100 Procrastinating Pats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this list, the number of Procrastinating Pats that we expect to find on each website after long times.\n",
    "Putting them in order of *popularity* (based on this metric), the PageRank of this micro-internet is:\n",
    "\n",
    "**C**atBabel, **D**romeda, **A**vocado, **F**aceSpace, **B**ullseye, **e**Tings\n",
    "\n",
    "Referring back to the micro-internet diagram, is this what we would have expected?\n",
    "Convince ourselves that based on which pages seem important given which others link to them, that this is a sensible ranking.\n",
    "\n",
    "Let's now try to get the same result using the Power-Iteration method that was covered in the video.\n",
    "This method will be much better at dealing with large systems.\n",
    "\n",
    "First let's set up our initial vector, $\\mathbf{r}^{(0)}$, so that we have our 100 Procrastinating Pats equally distributed on each of our 6 websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.66666667, 16.66666667, 16.66666667, 16.66666667, 16.66666667,\n",
       "       16.66666667])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 100 * np.ones(6) / 6 # Sets up this vector (6 entries of 1/6 × 100 each)\n",
    "r # Shows it's value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's update the vector to the next minute, with the matrix $L$.\n",
    "Run the following cell multiple times, until the answer stabilises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.9993165 ,  5.33383306, 39.99973582, 25.33405826,  0.        ,\n",
       "       13.33305637])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = L @ r # Apply matrix L to r\n",
    "r # Show it's value\n",
    "# Re-run this cell multiple times to converge to the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automate applying this matrix multiple times as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.        ,  5.33333333, 40.        , 25.33333333,  0.        ,\n",
       "       13.33333333])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 100 * np.ones(6) / 6 # Sets up this vector (6 entries of 1/6 × 100 each)\n",
    "for i in np.arange(100) : # Repeat 100 times\n",
    "    r = L @ r\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even better, we can keep running until we get to the required tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 iterations to convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([16.00149917,  5.33252025, 39.99916911, 25.3324738 ,  0.        ,\n",
       "       13.33433767])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 100 * np.ones(6) / 6 # Sets up this vector (6 entries of 1/6 × 100 each)\n",
    "lastR = r\n",
    "r = L @ r\n",
    "i = 0\n",
    "while la.norm(lastR - r) > 0.01 :\n",
    "    lastR = r\n",
    "    r = L @ r\n",
    "    i += 1\n",
    "print(str(i) + \" iterations to convergence.\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the PageRank order is established fairly quickly, and the vector converges on the value we calculated earlier after a few tens of repeats.\n",
    "\n",
    "Congratulations! We've just calculated our first PageRank!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damping Parameter\n",
    "The system we just studied converged fairly quickly to the correct answer.\n",
    "Let's consider an extension to our micro-internet where things start to go wrong.\n",
    "\n",
    "Say a new website is added to the micro-internet: *Geoff's* Website.\n",
    "This website is linked to by *FaceSpace* and only links to itself.\n",
    "![An Expanded Micro-Internet](internet2.png \"An Expanded Micro-Internet\")\n",
    "\n",
    "Intuitively, only *FaceSpace*, which is in the bottom half of the page rank, links to this website amongst the two others it links to,\n",
    "so we might expect *Geoff's* site to have a correspondingly low PageRank score.\n",
    "\n",
    "Build the new $L$ matrix for the expanded micro-internet, and use Power-Iteration on the Procrastinating Pat vector.\n",
    "See what happens…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We'll call this one L2, to distinguish it from the previous L.\n",
    "L2 = np.array([[0,   1/2, 1/3, 0, 0,   0, 0 ],\n",
    "               [1/3, 0,   0,   0, 1/2, 0, 0 ],\n",
    "               [1/3, 1/2, 0,   1, 0,   1/3, 0 ],\n",
    "               [1/3, 0,   1/3, 0, 1/2, 1/3, 0 ],\n",
    "               [0,   0,   0,   0, 0,   0, 0 ],\n",
    "               [0,   0,   1/3, 0, 0,   0, 0 ],\n",
    "               [0,   0,   0,   0, 0,   1/3, 1 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 iterations to convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03046998,  0.01064323,  0.07126612,  0.04423198,  0.        ,\n",
       "        0.02489342, 99.81849527])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 100 * np.ones(7) / 7 # Sets up this vector (7 entries of 1/7 × 100 each)\n",
    "lastR = r\n",
    "r = L2 @ r\n",
    "i = 0\n",
    "while la.norm(lastR - r) > 0.01 :\n",
    "    lastR = r\n",
    "    r = L2 @ r\n",
    "    i += 1\n",
    "print(str(i) + \" iterations to convergence.\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's no good! *Geoff* seems to be taking all the traffic on the micro-internet, and somehow coming at the top of the PageRank.\n",
    "This behaviour can be understood, because once a Pat get's to *Geoff's* Website, they can't leave, as all links head back to Geoff.\n",
    "\n",
    "To combat this, we can add a small probability that the Procrastinating Pats don't follow any link on a webpage, but instead visit a website on the micro-internet at random.\n",
    "We'll say the probability of them following a link is $d$ and the probability of choosing a random website is therefore $1-d$.\n",
    "We can use a new matrix to work out where the Pat's visit each minute.\n",
    "$$ M = d \\, L + \\frac{1-d}{n} \\, J $$\n",
    "where $J$ is an $n\\times n$ matrix where every element is one.\n",
    "\n",
    "If $d$ is one, we have the case we had previously, whereas if $d$ is zero, we will always visit a random webpage and therefore all webpages will be equally likely and equally ranked.\n",
    "For this extension to work best, $1-d$ should be somewhat small - though we won't go into a discussion about exactly how small.\n",
    "\n",
    "Let's retry this PageRank with this extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0.5 # Feel free to play with this parameter after running the code once.\n",
    "M = d * L2 + (1-d)/7 * np.ones([7, 7]) # np.ones() is the J matrix, with ones for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 iterations to convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([13.68217054, 11.20902965, 22.41964343, 16.7593433 ,  7.14285714,\n",
       "       10.87976354, 17.90719239])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 100 * np.ones(7) / 7 # Sets up this vector (7 entries of 1/7 × 100 each)\n",
    "lastR = r\n",
    "r = M @ r\n",
    "i = 0\n",
    "while la.norm(lastR - r) > 0.01 :\n",
    "    lastR = r\n",
    "    r = M @ r\n",
    "    i += 1\n",
    "print(str(i) + \" iterations to convergence.\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is certainly better, the PageRank gives sensible numbers for the Procrastinating Pats that end up on each webpage.\n",
    "This method still predicts Geoff has a high ranking webpage however.\n",
    "This could be seen as a consequence of using a small network. We could also get around the problem by not counting self-links when producing the L matrix (an if a website has no outgoing links, make it link to all websites equally).\n",
    "We won't look further down this route, as this is in the realm of improvements to PageRank, rather than eigenproblems.\n",
    "\n",
    "We are now in a good position, having gained an understanding of PageRank, to produce our own code to calculate the PageRank of a website with thousands of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Assessment\n",
    "In this assessment, we will be asked to produce a function that can calculate the PageRank for an arbitrarily large probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "# Here are the imports again, just in case we need them.\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from PageRankFunctions import *\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the PageRank for an arbitrarily sized internet.\n",
    "# I.e. the principal eigenvector of the damped system, using the power iteration method.\n",
    "# (Normalisation doesn't matter here)\n",
    "# The functions inputs are the linkMatrix, and d the damping parameter - as defined in this worksheet.\n",
    "# (The damping parameter, d, will be set by the function - no need to set this ourselves.)\n",
    "def pageRank(linkMatrix, d) :\n",
    "    n = linkMatrix.shape[0]\n",
    "    M = d * linkMatrix + (1-d)/n * np.ones([n, n]) # np.ones() is the J matrix, with ones for each entry.\n",
    "    r = 100 * np.ones(n) / n # Sets up this vector (n entries of 1/n × 100 each)\n",
    "    lastR = r\n",
    "    r = M @ r\n",
    "    i = 0\n",
    "    while la.norm(lastR - r) > 0.01 :\n",
    "        lastR = r\n",
    "        r = M @ r\n",
    "        i += 1\n",
    "    print(str(i) + \" iterations to convergence.\")\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our code\n",
    "We can use the code below to test out our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0.2, 0.2, 0.2, 0. ],\n",
       "       [0. , 0.2, 0.2, 0.2, 0. ],\n",
       "       [0. , 0.2, 0.2, 0.2, 0. ],\n",
       "       [0. , 0.2, 0.2, 0.2, 0.5],\n",
       "       [0. , 0.2, 0.2, 0.2, 0.5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following function to generate internets of different sizes.\n",
    "generate_internet(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our PageRank method against the built in \"eig\" method.\n",
    "# We should see ours is a lot faster for large internets\n",
    "L = generate_internet(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 iterations to convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00229915,  0.00229915,  0.00505316,  0.00954256,  0.00229915,\n",
       "       99.95778466,  0.00229915,  0.00954256,  0.00382732,  0.00505316])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageRank(L, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000003,  0.00000003,  0.00000007,  0.0000001 ,  0.00000003,\n",
       "       99.99999948,  0.00000003,  0.0000001 ,  0.00000005,  0.00000007])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do note, this is calculating the eigenvalues of the link matrix, L,\n",
    "# without any damping. It may give different results that our pageRank function.\n",
    "# If we wish, we could modify this cell to include damping.\n",
    "eVals, eVecs = la.eig(L) # Gets the eigenvalues and vectors\n",
    "order = np.absolute(eVals).argsort()[::-1] # Orders them by their eigenvalues\n",
    "eVals = eVals[order]\n",
    "eVecs = eVecs[:,order]\n",
    "\n",
    "r = eVecs[:, 0]\n",
    "100 * np.real(r / np.sum(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "37 iterations to convergence.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We may wish to view the PageRank graphically.\n",
    "# This code will draw a bar chart, for each (numbered) website on the generated internet,\n",
    "# The height of each bar will be the score in the PageRank.\n",
    "# Run this code to see the PageRank for each internet we generate.\n",
    "# Hopefully we should see what we might expect\n",
    "# - there are a few clusters of important websites, but most on the internet are rubbish!\n",
    "%pylab inline\n",
    "r = pageRank(generate_internet(100), 0.9)\n",
    "plt.bar(arange(r.shape[0]), r);"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "linear-algebra-machine-learning",
   "graded_item_id": "Sfbnp",
   "launcher_item_id": "aPxf3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
